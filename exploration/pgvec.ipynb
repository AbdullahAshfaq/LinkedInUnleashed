{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing job descriptions\n",
    "\n",
    "What does data work at IBM look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_company = 'IBM'\n",
    "target_position = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = '..'\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.creds import *\n",
    "from src.utils import neo4j_utils, mongo_utils, sql_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "match (c:Company)-[r:EMPLOYEE_IN]-(p)\n",
    "where r.title =~ '(?i).*{target_position}.*' and c.name='{target_company}'\n",
    "return *;\n",
    "'''\n",
    "\n",
    "neoCon = neo4j_utils.neo_connector(creds=creds['neo_creds'])\n",
    "info, result = neoCon.runQuery_returnObj(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Node element_id='129543' labels=frozenset({'Company'}) properties={'company_id': '1009', 'name': 'IBM'}>,\n",
       " <Node element_id='166797' labels=frozenset({'People'}) properties={'public_identifier': 'aaron-christopher-lee', 'full_name': 'Aaron Lee'}>,\n",
       " <Relationship element_id='271768' nodes=(<Node element_id='166797' labels=frozenset({'People'}) properties={'public_identifier': 'aaron-christopher-lee', 'full_name': 'Aaron Lee'}>, <Node element_id='129543' labels=frozenset({'Company'}) properties={'company_id': '1009', 'name': 'IBM'}>) type='EMPLOYEE_IN' properties={'description': 'Designed and implemented a new Watson Cloud offering that uses speech recognition to automatically caption video content using Python and Node.js.', 'starts_at': '2017-5-1', 'is_active': False, 'ends_at': '2017-8-31', 'title': 'Extreme Blue Data Science Intern - Watson'}>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_lst = []\n",
    "for r in result:\n",
    "    desc = r[2].get('description')\n",
    "    description_lst.append(desc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Designed and implemented a new Watson Cloud offering that uses speech recognition to automatically caption video content using Python and Node.js.',\n",
       " '07/2019 - Current: Global Sales Incentives - E2E Data Integration Expert (Global NY - Remote MTL)\\n- Data integration expert for Incentives and Commissions: Big data management, data validation process and tools, report development with focus on analytics and education on all data relationships available for the systems.\\n- Responsible for creating innovative cognitive-type solutions for data quality. This includes complex ad hoc automated queries using SQL, JS and DataStage as well as the development of new web based tools in Bluemix using Python. \\n\\n01/2014 - 07/2019: Chief Data Office - Transformation & Operations - Senior Data Analyst (Global - Remote MTL) \\n- Responsible for WW CMR data quality and reporting/maintenance processes.\\n- Collaborates with key stakeholders in order to understand their needs and deliver value beyond our organization by providing client data solutions that benefit all of IBM.\\n- Creates innovative SQL solutions that gather large datasets for data collection and data cleansing in order to manipulate and analyze data to drive solutions for stakeholders. \\n- Responsible for creating innovative solutions for data quality. This includes complex ad hoc automated queries as well as the development of new web based tools.\\n\\n04/2007 - 12/2013: Sales Transaction Support - Customer Master Records - Coverage SME  (Global - Remote MTL)\\n\\n04/2005 - 04/2007: Incentives Financial and Process Management - Commissions Business Analyst (MTL)\\n\\n04/2004 - 04/2005: Global Services - Financial Analyst & Pricer (MTL)\\n\\n10/2003 - 04/2004: Global Financing - Special Bids Pricer (MTL)\\n\\n09/2000 - 10/2003: Call Centre Technical Support - Call Centre Coordinator (MTL)\\n\\n07/1998 - 09/2000: Call Centre Technical Support - Team Leader (MTL)\\n\\n07/1997 - 07/1998: Call Centre Technical Support - Education Coordinator (Toronto)\\n\\n01/1996 - 07/1997: Call Centre Technical Support - Lead Product Specialist (MTL)\\n\\n09/1995 - 01/1996: Call Centre Technical Support - Product Specialist (MTL)',\n",
       " '',\n",
       " '* Develop Watson health cognitive framework for image analytics to support various advisors such as breast advisor, heart advisor.\\n* Analyze the algorithms for detection of cancer which uses deep learning algorithms in computer vision.\\n* Train the machine learning classifier models to detect medical imaging modalities.\\n* Automate the end-to-end flow of data from Hospital servers to drive the Machine learning models and persist the results in the database.\\n* Code the custom processors in NiFi to achieve user-defined data transformation logic.\\n* Design data models for different advisors.\\n* Design the retraining machine learning algorithms to improve the accuracy of trained models.',\n",
       " 'Contributed in co-creation projects to develop cognitive solutions and services for the Cognitive Enterprise Data Platform (CEDP).',\n",
       " '',\n",
       " 'Database development and Administration for all Microsoft SQL Server instances owned by Ogilvy Commonhealth World Wide.\\nDeveloped Stored Procedures on back end databases for major Financial Services project.\\nPerformance Tuning of SQL Server.\\nQuery Tuning\\nAutomation using Red Gate tools and Powershell.\\nDeveloped HA/DR solutions using AlwaysOn Availability Groups.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'postgresql+psycopg2://postgres:postgres@localhost:5439/postgresdb'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONNECTION_STRING = f\"postgresql+psycopg2://{creds['sql_creds']['user']}:{creds['sql_creds']['password']}@{creds['sql_creds']['host']}:{creds['sql_creds']['port']}/{creds['sql_creds']['db']}\"\n",
    "CONNECTION_STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\D_drive\\Tutorials\\Pytorch_basics\\pytorch_v2.1.0\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# This will download model. Rerun this cell if download is not 100% completed. If all components are not 100% downloaded, the following steps will fail\n",
    "embeddings =  HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorstores(text_chunks, COLLECTION_NAME, CONNECTION_STRING):\n",
    "    # This downloads the embeddings on system. So we are not sending anything to Huggingface\n",
    "    embeddings =  HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-large')\n",
    "    vectorstore = PGVector.from_texts(embedding=embeddings,\n",
    "                    texts = text_chunks,\n",
    "                    collection_name=COLLECTION_NAME,\n",
    "                    connection_string=CONNECTION_STRING)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "\n",
    "def get_llm():\n",
    "    \"\"\"\n",
    "        get_conversation_chain is called once so this function should also be called once\n",
    "    \"\"\"\n",
    "    model_id = \"google/flan-t5-large\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=400,\n",
    "        do_sample=True,\n",
    "        top_k=1,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline = pipeline)\n",
    "\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "def get_conversation_chain(vectorstore):\n",
    "    \n",
    "    # since chatbot has memory, we initialize instance of memory\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    # llm = ChatOpenAI() # Only works if OpenAI subscription\n",
    "    llm = get_llm()\n",
    "\n",
    "    conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conv_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def get_text_chunks(raw_text):\n",
    "    \"Returns list of text chunks\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator='\\n', \n",
    "        chunk_size = 500, # 1000 characters\n",
    "        chunk_overlap = 50, # To keep redundant information if chunk ends abruptly\n",
    "        length_function = len\n",
    "        )\n",
    "    chunks = text_splitter.split_text(raw_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_chunks = get_text_chunks(';'.join(description_lst))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing extracted descriptions to LLM to see what are the significant features of this position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CONNECTION_STRING = f\"postgresql+psycopg2://{creds['sql_creds']['user']}:{creds['sql_creds']['password']}@{creds['sql_creds']['host']}:{creds['sql_creds']['port']}/{creds['sql_creds']['db']}\"\n",
    "COLLECTION_NAME = 'ds_descri'\n",
    "\n",
    "vectorstore = get_vectorstores(desc_chunks, COLLECTION_NAME, CONNECTION_STRING)\n",
    "conversation = get_conversation_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"List the technologies used in these roles?\"\n",
    "response = conversation({'question': user_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SQL, JS and DataStage as well as the development of new web based tools in Bluemix using Python'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical NLP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Watson Cloud,\n",
       " Node.js.;07/2019,\n",
       " Incentives,\n",
       " SQL,\n",
       " JS,\n",
       " DataStage,\n",
       " Bluemix,\n",
       " Office - Transformation & Operations - Senior Data Analyst,\n",
       " Global - Remote,\n",
       " IBM,\n",
       " SQL,\n",
       " SME,\n",
       " Global - Remote,\n",
       " MTL,\n",
       " Global Services - Financial Analyst & Pricer,\n",
       " MTL,\n",
       " 10/2003 - 04/2004,\n",
       " Global Financing - Special Bids Pricer,\n",
       " MTL,\n",
       " Call Centre Technical Support - Call Centre Coordinator,\n",
       " MTL,\n",
       " 07/1998 - 09/2000,\n",
       " Call Centre Technical Support - Team,\n",
       " MTL,\n",
       " Call Centre Technical Support - Education Coordinator,\n",
       " Toronto,\n",
       " Call Centre Technical Support - Lead Product Specialist,\n",
       " MTL,\n",
       " Call Centre Technical Support - Product Specialist,\n",
       " MTL,\n",
       " Develop Watson,\n",
       " Hospital,\n",
       " Machine,\n",
       " NiFi,\n",
       " the Cognitive Enterprise Data,\n",
       " Microsoft,\n",
       " Ogilvy Commonhealth World Wide,\n",
       " Developed Stored Procedures,\n",
       " Financial Services,\n",
       " SQL Server,\n",
       " Red Gate,\n",
       " Powershell,\n",
       " Developed HA/DR,\n",
       " AlwaysOn Availability Groups)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = ';'.join(description_lst)\n",
    "doc = nlp(text)\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Watson Cloud,\n",
       " Node.js.;07/2019,\n",
       " Incentives,\n",
       " SQL,\n",
       " JS,\n",
       " DataStage,\n",
       " Bluemix,\n",
       " Office - Transformation & Operations - Senior Data Analyst,\n",
       " Global - Remote,\n",
       " IBM)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
